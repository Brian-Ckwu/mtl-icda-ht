{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import gc\n",
    "sys.path.append(\"/nfs/nas-7.1/ckwu/mtl-icda-ht\")\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import ttest_rel\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from utilities.data import MedicalDxDataset, DxBatchCollator, MedicalDxNERIOBDataset, convert_icds_to_indices, split_by_div\n",
    "from utilities.model import BertDxModel, BertDxNERModel, encoder_names_mapping\n",
    "from utilities.utils import move_bert_input_to_device, set_seeds\n",
    "from utilities.evaluation import predict_whole_set_dx, get_top_k_accuracies, get_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Configuration\n",
    "\"\"\"\n",
    "config = json.loads(Path(\"./dx_config.json\").read_bytes())\n",
    "args = Namespace(**config)\n",
    "\n",
    "if \"cuda\" in args.device:\n",
    "    assert torch.cuda.is_available()\n",
    "\n",
    "set_seeds(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs = pickle.loads(Path(args.emr_path).read_bytes())\n",
    "icds = pickle.loads(Path(args.dx_path).read_bytes())\n",
    "icd_ids = convert_icds_to_indices(icds, full_code=args.fc)\n",
    "\n",
    "# Validation split\n",
    "data_l = [emrs, icd_ids]\n",
    "valid_emrs, valid_dxs = [split_by_div(data, args.fold, args.remainder, mode=\"valid\") for data in data_l]\n",
    "\n",
    "# Dataset & DataLoader\n",
    "tokenizer = BertTokenizerFast.from_pretrained(encoder_names_mapping[args.tokenizer])\n",
    "collate_fn = DxBatchCollator(tokenizer)\n",
    "valid_set = MedicalDxDataset(valid_emrs, valid_dxs, tokenizer)\n",
    "valid_loader = DataLoader(valid_set, args.bs, shuffle=False, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 768\n",
    "LABEL_SIZE = 97\n",
    "\n",
    "model = BertDxModel(\n",
    "    model_name=encoder_names_mapping[args.encoder],\n",
    "    embed_size=EMBED_SIZE,\n",
    "    label_size=LABEL_SIZE\n",
    ")\n",
    "model.load_state_dict(torch.load(args.ckpt_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict_whole_set_dx(model, valid_loader, args.device).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_accuracies(valid_dxs, preds, k=9, labels=range(LABEL_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Configuration\n",
    "\"\"\"\n",
    "config = json.loads(Path(\"./dx_ner_config.json\").read_bytes())\n",
    "args = Namespace(**config)\n",
    "\n",
    "set_seeds(args.seed)\n",
    "\n",
    "\"\"\"\n",
    "    Data\n",
    "\"\"\"\n",
    "emrs = pickle.loads(Path(args.emr_path).read_bytes())\n",
    "icds = pickle.loads(Path(args.dx_path).read_bytes())\n",
    "icd_ids = convert_icds_to_indices(icds, full_code=args.fc)\n",
    "ner_spans_l = pickle.loads(Path(args.ner_spans_l_path).read_bytes())\n",
    "data_l = [emrs, icd_ids, ner_spans_l]\n",
    "\"\"\"\n",
    "    Others\n",
    "\"\"\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(encoder_names_mapping[args.tokenizer])\n",
    "# collate_fn = DxBatchCollator(tokenizer)\n",
    "# EMBED_SIZE = 768\n",
    "DX_LABEL_SIZE = 97\n",
    "NER_LABEL_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_acc_dfs = list()\n",
    "eval_dfs = list()\n",
    "\n",
    "for k in range(args.fold):\n",
    "    print(f\"Start evaluating fold = {k}:\\n\")\n",
    "    # Config\n",
    "    args.ckpt_path = re.sub(pattern=r\"remainder\\-\\d\", repl=f\"remainder-{k}\", string=args.ckpt_path)\n",
    "\n",
    "    # Data\n",
    "    valid_emrs, valid_dxs, valid_ners = [split_by_div(data, args.fold, remainder=k, mode=\"valid\") for data in data_l]\n",
    "    valid_set = MedicalDxNERIOBDataset(valid_emrs, valid_dxs, valid_ners, tokenizer)\n",
    "    valid_loader = DataLoader(valid_set, args.bs, shuffle=False, pin_memory=True, collate_fn=valid_set.collate_fn)\n",
    "\n",
    "    # Model\n",
    "    model = BertDxNERModel(\n",
    "        encoder=encoder_names_mapping[args.encoder],\n",
    "        dx_label_size=DX_LABEL_SIZE,\n",
    "        ner_label_size=NER_LABEL_SIZE,\n",
    "        loss_weights=args.lw\n",
    "    )\n",
    "    model.load_state_dict(torch.load(args.ckpt_path))\n",
    "\n",
    "    # Evaluation\n",
    "    scores = predict_whole_set_dx(model, valid_loader, args.device).detach().cpu()\n",
    "    preds = scores.argmax(dim=-1)\n",
    "    top_k_acc_df = get_top_k_accuracies(valid_dxs, scores, k=9, labels=range(DX_LABEL_SIZE))\n",
    "    top_k_acc_dfs.append(top_k_acc_df)\n",
    "    eval_df = get_evaluations(valid_dxs, preds, DX_LABEL_SIZE, scores, args.encoder)\n",
    "    eval_dfs.append(eval_df)\n",
    "\n",
    "    del valid_emrs, valid_dxs, valid_set, valid_loader, model, scores, preds, top_k_acc_df, eval_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-k Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_acc_dfs = [top_k_acc_dfs[i].T for i in range(args.fold)]\n",
    "top_k_acc_dfs_cat = pd.concat(top_k_acc_dfs)\n",
    "\n",
    "acc_mean_df = top_k_acc_dfs_cat.mean(axis=0).to_frame().T.rename({0: \"mean\"})\n",
    "acc_std_df = top_k_acc_dfs_cat.std(axis=0).to_frame().T.rename({0: \"std\"})\n",
    "acc_mean_std_df = pd.concat(objs=[acc_mean_df, acc_std_df])\n",
    "\n",
    "acc_mean_std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dfs_cat = pd.concat(eval_dfs)\n",
    "\n",
    "eval_mean_df = eval_dfs_cat.mean(axis=0).to_frame().T.rename({0: \"mean\"})\n",
    "eval_std_df = eval_dfs_cat.std(axis=0).to_frame().T.rename({0: \"std\"})\n",
    "eval_mean_std_df = pd.concat(objs=[eval_mean_df, eval_std_df])\n",
    "\n",
    "eval_mean_std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.eval_save_dir = Path(args.eval_save_dir)\n",
    "args.eval_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "acc_save_dir = args.eval_save_dir / f\"{args.encoder}_top_k_acc_mean_std.csv\"\n",
    "eval_save_dir = args.eval_save_dir / f\"{args.encoder}_eval_mean_std.csv\"\n",
    "\n",
    "acc_mean_std_df.to_csv(acc_save_dir, index_label=\"index\")\n",
    "eval_mean_std_df.to_csv(eval_save_dir, index_label=\"index\")\n",
    "\n",
    "loaded_acc_df = pd.read_csv(acc_save_dir, index_col=\"index\")\n",
    "loaded_eval_df = pd.read_csv(eval_save_dir, index_col=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_load = Path(\"/nfs/nas-7.1/ckwu/mtl-icda-ht/comparisons/eval_results/dx/ClinicalBERT_eval_mean_std.csv\")\n",
    "\n",
    "df = pd.read_csv(to_load, index_col=\"index\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Observe Evaluations\n",
    "\"\"\"\n",
    "eval_results = {\n",
    "    \"dx\": list(),\n",
    "    \"dx_ner\": list()\n",
    "}\n",
    "\n",
    "def combine_evaluations(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    final_df = None\n",
    "    for i, df in enumerate(dfs):\n",
    "        df = df.rename({\"acc\": f\"remainder-{i}\"}, axis=1).T\n",
    "        if final_df is None:\n",
    "            final_df = df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df, df], axis=0)\n",
    "    return final_df\n",
    "\n",
    "for model_type in [\"dx\", \"dx_ner\"]:\n",
    "    for r in range(config[\"fold\"]):\n",
    "        file_path = f\"./eval_results/{model_type}/{config['fold']}-fold/remainder-{r}_top_k_accuracies.csv\"\n",
    "        with open(file_path) as f:\n",
    "            df = pd.read_csv(f, index_col=\"k\")\n",
    "        eval_results[model_type].append(df)\n",
    "\n",
    "dx_result = combine_evaluations(eval_results[\"dx\"])\n",
    "mtl_result = combine_evaluations(eval_results[\"dx_ner\"])\n",
    "\n",
    "dx_mean = dx_result.mean(axis=0).to_frame().T\n",
    "mtl_mean = mtl_result.mean(axis=0).to_frame().T\n",
    "comparison  = pd.concat([dx_mean, mtl_mean], ignore_index=True).rename(mapper={0: \"dx\", 1: \"dx_ner\"}, axis=0)\n",
    "\n",
    "p_values = list()\n",
    "for k in range(1, 10):\n",
    "    a = mtl_result[k].values\n",
    "    b = dx_result[k].values\n",
    "    p = ttest_rel(a, b, alternative=\"greater\")[1]\n",
    "    p_values.append(p)\n",
    "\n",
    "p_values_df = pd.DataFrame(p_values).T.rename(mapper=lambda c: c + 1, axis=1)\n",
    "comparison = pd.concat([comparison, p_values_df]).rename(mapper={0: \"p_value\"}, axis=0)\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis single task accuracy\n",
    "best_val_accs = list()\n",
    "\n",
    "for r in range(10):\n",
    "    ckpt_path = Path(f\"/nfs/nas-7.1/ckwu/mtl-icda-ht/components_testing/diagnosis/eval_results/encoder-BioBERT_dx-97_lr-4e-05_remainder-{r}.json\")\n",
    "    train_log = json.loads(ckpt_path.read_bytes())\n",
    "    best_val_acc = train_log[\"best_val_acc\"]\n",
    "    best_val_accs.append(best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "best_val_accs = np.array(best_val_accs)\n",
    "best_val_accs.mean(), best_val_accs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER single task accuracy\n",
    "single_ner_accs = list()\n",
    "\n",
    "for r in range(10):\n",
    "    ckpt_path = Path(f\"/nfs/nas-7.1/ckwu/mtl-icda-ht/components_testing/ner/eval_results/encoder-BioBERT_nepochs-10_bs-16_lr-5e-05_fold-10_remainder-{r}.txt\")\n",
    "    ner_acc = float(ckpt_path.read_text().rstrip())\n",
    "    single_ner_accs.append(ner_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_ner_accs = np.array(single_ner_accs)\n",
    "single_ner_accs.mean(), single_ner_accs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# lws = [f\"{d}.0\" for d in range(5, 9)]\n",
    "# lrs = [\"4e-05\"]\n",
    "stps = [3, 4, 5]\n",
    "\n",
    "# dx_acc_comps = {lr: list() for lr in lrs}\n",
    "# ner_acc_comps = {lr: list() for lr in lrs}\n",
    "dx_acc_comps = {stp: list() for stp in stps}\n",
    "ner_acc_comps = {stp: list() for stp in stps}\n",
    "\n",
    "# for lw in lws:\n",
    "for stp in stps:\n",
    "    best_dx_accs = list()\n",
    "    best_ner_accs = list()\n",
    "    for r in range(10):\n",
    "        ckpt_path = Path(f\"/nfs/nas-7.1/ckwu/mtl-icda-ht/multitask_models/diagnosis_ner/models_separate_update/encoder-BioBERT_fc-False_lw-6.0_nersteps-{stp}_lr-4e-05_remainder-{r}/train_log.json\")\n",
    "        train_log = json.loads(ckpt_path.read_bytes())\n",
    "        best_dx_accs.append(train_log[\"best_dx_acc\"])\n",
    "        best_ner_accs.append(train_log[\"best_ner_acc\"])\n",
    "    mean_dx_acc = sum(best_dx_accs) / len(best_dx_accs)\n",
    "    mean_ner_acc = sum(best_ner_accs) / len(best_ner_accs)\n",
    "\n",
    "    dx_acc_comps[stp].append(mean_dx_acc)\n",
    "    ner_acc_comps[stp].append(mean_ner_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "best_dx_accs = np.array(best_dx_accs)\n",
    "best_dx_accs.mean(), best_dx_accs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ner_accs = np.array(best_ner_accs)\n",
    "best_ner_accs.mean(), best_ner_accs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nerstps lw-6.0\n",
    "dx_acc_comps, ner_acc_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3e-5 no Aho\n",
    "dx_acc_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Aho-Cora\n",
    "dx_acc_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(best_dx_accs) / len(best_dx_accs), sum(best_ner_accs) / len(best_ner_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure NER\n",
    "best_pure_ner_accs = list()\n",
    "for r in range(10):\n",
    "    ckpt_path = Path(f\"/nfs/nas-7.1/ckwu/mtl-icda-ht/components_testing/ner/eval_results/encoder-BioBERT_nepochs-10_bs-16_lr-5e-05_fold-10_remainder-{r}.txt\")\n",
    "    best_pure_ner_acc = float(ckpt_path.read_text().rstrip())\n",
    "    best_pure_ner_accs.append(best_pure_ner_acc)\n",
    "\n",
    "sum(best_pure_ner_accs) / len(best_pure_ner_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_files_prefix = {\n",
    "    \"ner\": \"../components_testing/ner/eval_results/nepochs-5_fold-10_remainder\",\n",
    "    \"dx_ner\": \"../multitask_models/diagnosis_ner/eval_results/mtl_lw-1.0-9.0_fold-10_lr5e-5_remainder\"\n",
    "}\n",
    "\n",
    "eval_results = {\n",
    "    \"ner\": list(),\n",
    "    \"dx_ner\": list()\n",
    "}\n",
    "\n",
    "folds = 10\n",
    "for key in eval_results.keys():\n",
    "    for remainder in range(folds):\n",
    "        file = f\"{eval_files_prefix[key]}-{remainder}.json\"\n",
    "        with open(file) as f:\n",
    "            result = json.load(f)\n",
    "        ner_acc = result[\"best_val_acc\"] if key == \"ner\" else result[\"best_ner_acc\"]\n",
    "        eval_results[key].append(ner_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_mean = sum(eval_results[\"ner\"]) / len(eval_results[\"ner\"])\n",
    "mtl_mean = sum(eval_results[\"dx_ner\"]) / len(eval_results[\"dx_ner\"])\n",
    "\n",
    "eval_results[\"dx_ner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_rel(a=eval_results[\"ner\"], b=eval_results[\"dx_ner\"], alternative=\"two-sided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[\"ner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[\"dx_ner\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/nfs/nas-7.1/ckwu/mtl-icda-ht/multitask_models/diagnosis_ner/eval_results/encoder-BioBERT_dx-97_ner-2_lw-1.0-8.0_lr-5e-05_remainder-0.json\"\n",
    "with open(file) as f:\n",
    "    train_log = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_acc = train_log[\"dx\"][\"acc\"]\n",
    "ner_acc = train_log[\"ner\"][\"acc\"]\n",
    "\n",
    "dx_loss = train_log[\"dx\"][\"loss\"]\n",
    "ner_loss = train_log[\"ner\"][\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_ner_comp = pd.DataFrame(data={\n",
    "    \"dx_acc\": dx_acc,\n",
    "    \"ner_acc\": ner_acc,\n",
    "    \"dx_loss\": dx_loss,\n",
    "    \"ner_loss\": ner_loss\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_ner_comp[\"dx_acc\"].plot()\n",
    "dx_ner_comp[\"ner_acc\"].plot(secondary_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_ner_comp[\"dx_loss\"].plot()\n",
    "dx_ner_comp[\"ner_loss\"].plot(secondary_y=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d36bacadd0665529eea11cf0e685dbe0df5591baeef20597c1b063990f001dc3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
