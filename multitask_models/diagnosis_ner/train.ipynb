{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from utilities.data import MedicalDxNERIOBDataset, convert_icds_to_indices, split_by_div\n",
    "from utilities.utils import set_seeds, render_exp_name, move_bert_input_to_device\n",
    "from utilities.model import BertDxNERModel, encoder_names_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save(data_loader, model, args, train_log):\n",
    "    dx_correct, dx_predict, ner_correct, ner_predict = 0, 0, 0, 0\n",
    "    total_dx_loss, total_ner_loss = 0.0, 0.0\n",
    "\n",
    "    model.eval()\n",
    "    for x, y_dx, y_ner in data_loader:\n",
    "        x = move_bert_input_to_device(x, args.device)\n",
    "        y_dx = y_dx.to(args.device)\n",
    "        y_ner = y_ner.to(args.device)\n",
    "        with torch.no_grad():\n",
    "            o_dx, o_ner = model(x)\n",
    "            dx_loss, ner_loss, _ = model.calc_loss(o_dx, y_dx, o_ner, y_ner)\n",
    "            # dx acc\n",
    "            dx_correct += (o_dx.argmax(dim=-1) == y_dx).sum().cpu().detach().item()\n",
    "            dx_predict += len(y_dx)\n",
    "            # ner acc (token acc)\n",
    "            ner_correct += (o_ner.argmax(dim=-1) == y_ner).sum().cpu().detach().item()\n",
    "            ner_predict += (y_ner != model.ner_ignore_index).sum().cpu().detach().item()\n",
    "            total_dx_loss += dx_loss.cpu().detach().item() * y_dx.shape[0]\n",
    "            total_ner_loss += ner_loss.cpu().detach().item() * y_ner.shape[0]\n",
    "\n",
    "    dx_acc = dx_correct / dx_predict\n",
    "    ner_acc = ner_correct / ner_predict\n",
    "    dx_loss = total_dx_loss / len(data_loader.dataset)\n",
    "    ner_loss = total_ner_loss / len(data_loader.dataset)\n",
    "    print(f\"Diagnosis: acc -> {dx_acc:.3f}; loss -> {dx_loss:.3f} / NER: acc -> {ner_acc:.3f}; loss -> {ner_loss:.3f}\")\n",
    "\n",
    "    # update train log\n",
    "    for key, value in zip([\"dx_acc\", \"dx_loss\", \"ner_acc\", \"ner_loss\"], [dx_acc, dx_loss, ner_acc, ner_loss]):\n",
    "        key_l = getattr(train_log, key)\n",
    "        key_l.append(value)\n",
    "    \n",
    "    # update best metrics and save\n",
    "    if dx_acc > train_log.best_dx_acc:\n",
    "        train_log.best_dx_acc = dx_acc\n",
    "        torch.save(model.state_dict(), args.ckpt_path / \"best_model.ckpt\")\n",
    "        print(f\"Best model saved. (Dx acc = {dx_acc:.3f}; NER token acc = {ner_acc:.3f}\")\n",
    "    if ner_acc > train_log.best_ner_acc:\n",
    "        train_log.best_ner_acc = ner_acc\n",
    "\n",
    "    # save train log\n",
    "    (args.ckpt_path / \"train_log.json\").write_text(data=json.dumps(vars(train_log)))\n",
    "    \n",
    "    return train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Configuration\n",
    "\"\"\"\n",
    "config = json.loads(Path(\"./config.json\").read_bytes())\n",
    "args = Namespace(**config)\n",
    "\n",
    "args.exp_name = render_exp_name(args, hparams=[\"encoder\", \"fc\", \"lw\", \"lr\", \"remainder\"])\n",
    "\n",
    "args.ckpt_path = Path(args.save_dir) / args.exp_name\n",
    "args.ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "(args.ckpt_path / \"args.pickle\").write_bytes(data=pickle.dumps(args))\n",
    "\n",
    "set_seeds(args.seed)\n",
    "if \"cuda\" in args.device:\n",
    "    assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Data\n",
    "\"\"\"\n",
    "# x: EMRs\n",
    "emrs = pickle.loads(Path(args.emr_path).read_bytes())\n",
    "# y: Dx\n",
    "dxs = pickle.loads(Path(args.dx_path).read_bytes())\n",
    "dx_labels = convert_icds_to_indices(dxs, full_code=args.fc)\n",
    "# y: NER\n",
    "spans_tuples = pickle.loads(Path(args.ner_spans_tuples_path).read_bytes())\n",
    "\n",
    "data_l = [emrs, dx_labels, spans_tuples]\n",
    "train_emrs, train_dxs, train_ners = [split_by_div(data, fold=args.fold, remainder=args.remainder, mode=\"train\") for data in data_l]\n",
    "valid_emrs, valid_dxs, valid_ners = [split_by_div(data, fold=args.fold, remainder=args.remainder, mode=\"valid\") for data in data_l]\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(encoder_names_mapping[args.tokenizer])\n",
    "train_set = MedicalDxNERIOBDataset(train_emrs, train_dxs, train_ners, tokenizer)\n",
    "valid_set = MedicalDxNERIOBDataset(valid_emrs, valid_dxs, valid_ners, tokenizer)\n",
    "train_loader = DataLoader(train_set, batch_size=args.bs, shuffle=True, pin_memory=True, collate_fn=train_set.collate_fn)\n",
    "valid_loader = DataLoader(valid_set, batch_size=args.bs, shuffle=True, pin_memory=True, collate_fn=valid_set.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Model\n",
    "\"\"\"\n",
    "model = BertDxNERModel(\n",
    "    encoder=encoder_names_mapping[args.encoder],\n",
    "    dx_label_size=train_set.num_dx_labels,\n",
    "    ner_label_size=train_set.num_ner_labels,\n",
    "    loss_weights=args.lw\n",
    ").to(args.device)\n",
    "optimizer = getattr(torch.optim, args.optimizer)(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Optimization\n",
    "\"\"\"\n",
    "train_log = Namespace(\n",
    "    dx_acc=list(),\n",
    "    dx_loss=list(),\n",
    "    ner_acc=list(),\n",
    "    ner_loss=list(),\n",
    "    best_dx_acc=0,\n",
    "    best_ner_acc=0\n",
    ")\n",
    "\n",
    "nsteps = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(args.nepochs):\n",
    "    print(f\"\\n===== Training at epoch {epoch + 1} =====\\n\")\n",
    "    for x, y_dx, y_ner in train_loader:\n",
    "        model.train()\n",
    "\n",
    "        # move to device\n",
    "        x = move_bert_input_to_device(x, args.device)\n",
    "        y_dx = y_dx.to(args.device)\n",
    "        y_ner = y_ner.to(args.device)\n",
    "\n",
    "        # inference\n",
    "        o_dx, o_ner = model(x)\n",
    "        dx_loss, ner_loss, total_loss = model.calc_loss(o_dx, y_dx, o_ner, y_ner)\n",
    "\n",
    "        # back-prop\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate model every k steps\n",
    "        if nsteps % args.ckpt_steps == 0:\n",
    "            print(f\"Evaluating model at step {nsteps}...\")\n",
    "            train_log = evaluate_and_save(valid_loader, model, args, train_log)\n",
    "            # save train_log\n",
    "        nsteps += 1\n",
    "\n",
    "    print(f\"----- Evaluating at epoch {epoch + 1} -----\")\n",
    "    train_log = evaluate_and_save(valid_loader, model, args, train_log)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d36bacadd0665529eea11cf0e685dbe0df5591baeef20597c1b063990f001dc3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
